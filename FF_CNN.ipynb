{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLjOry3pqmrV"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhR5nVEbqPX1",
        "outputId": "4d07c65e-178e-434f-9ba6-d99c342543c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Courses/CSCI566-DLA/Shashank/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq7YntjHZo6N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.datasets import SVHN\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import TensorDataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qxu18-3fLOA"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwQuXgjHPYqS"
      },
      "outputs": [],
      "source": [
        "def overlay_y_on_x(x, y, num_labels):\n",
        "    \"\"\"Replace the first num_label pixels of data [x] with one-hot-encoded label [y]\n",
        "    \"\"\"\n",
        "    x_ = x.clone()\n",
        "    x_[:, :num_labels] *= 0.0\n",
        "    x_[range(x.shape[0]), y] = x.max()\n",
        "    return x_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzsFv60rQETu"
      },
      "outputs": [],
      "source": [
        "class Convolutional_layer(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size: tuple, output_size: int, kernel_size: int, padding: int):\n",
        "    \"\"\"\n",
        "      input_size: n_h, n_w, n_c\n",
        "      output_size: int\n",
        "      patch_size: int\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.patch_size = kernel_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    n_h, n_w, n_c = input_size\n",
        "    patch_dim = n_h // kernel_size\n",
        "    self.patch_embedding = nn.Conv2d(n_c, output_size, kernel_size=kernel_size, stride=1, padding=padding, device=device)\n",
        "\n",
        "    self.lrelu = nn.ReLU() #nn.LeakyReLU()\n",
        "\n",
        "  def __normalize(self, x):\n",
        "    x_shape = x.shape\n",
        "    x = x.reshape(x_shape[0], -1)\n",
        "    x = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "    x = x.reshape(x_shape)\n",
        "    return x\n",
        "\n",
        "  def __reshape(self, x):\n",
        "    x = x.reshape(x.size(0), self.input_size[2], self.input_size[0], self.input_size[1])\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.__normalize(x) \n",
        "    # print(x.shape,'normalized')\n",
        "    x = self.__reshape(x) # (batch_size, channel_size, height, width)\n",
        "    x = self.patch_embedding(x) # (batch_size, 64, patch_dim, patch_dim)\n",
        "    # print(x.shape,'conv2d')\n",
        "\n",
        "\n",
        "    # x = self.__normalize(x)\n",
        "    x = self.lrelu(x)\n",
        "    # print(x.shape,'relu')\n",
        "\n",
        "    return x\n",
        "  \n",
        "  def train_ff(self, train_loader, epoch_range, batch_size, lr=0.03, threshold=2.0):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "    next_pos = []\n",
        "    next_neg = []\n",
        "    next_label = []\n",
        "    for e in epoch_range:\n",
        "      for (x_pos, x_neg, label) in train_loader:\n",
        "        x_pos = x_pos.to(device)\n",
        "        x_neg = x_neg.to(device)\n",
        "        g_pos = self.forward(x_pos).pow(2).mean(dim=1)\n",
        "        g_neg = self.forward(x_neg).pow(2).mean(dim=1)\n",
        "\n",
        "        loss = torch.log(1 + torch.exp(\n",
        "            torch.cat([\n",
        "                threshold - g_pos,\n",
        "                g_neg - threshold\n",
        "            ])\n",
        "        )).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        x_pos = x_pos.detach()\n",
        "        x_neg = x_neg.detach()\n",
        "\n",
        "    for (x_pos, x_neg, label) in train_loader:\n",
        "      x_pos = x_pos.to(device)\n",
        "      x_neg = x_neg.to(device)    \n",
        "      x_pos_next = self.forward(x_pos).detach()\n",
        "      x_neg_next = self.forward(x_neg).detach()\n",
        "      next_pos.append(x_pos_next)\n",
        "      next_neg.append(x_neg_next)\n",
        "      next_label.append(label)\n",
        "      x_pos = x_pos.detach()\n",
        "      x_neg = x_neg.detach()\n",
        "    \n",
        "    next_pos = torch.cat(next_pos, dim=0)\n",
        "    next_neg = torch.cat(next_neg, dim=0)\n",
        "    next_label = torch.cat(next_label, dim=0)\n",
        "\n",
        "\n",
        "    return DataLoader(TensorDataset(\n",
        "        next_pos,\n",
        "        next_neg,\n",
        "        next_label\n",
        "    ), batch_size=batch_size, shuffle=True)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXwpDQKDp2Fe"
      },
      "outputs": [],
      "source": [
        "class Flatten_layer(nn.Flatten):\n",
        "  def train_ff(self,train_loader, epoch_range, batch_size, lr=0.03, threshold=15.0):\n",
        "      next_pos = []\n",
        "      next_neg = []\n",
        "      next_label = []\n",
        "      for (x_pos, x_neg, label) in train_loader:\n",
        "        # print(x_pos.shape, 'xpos')\n",
        "        # print(x_neg.shape, 'xneg')\n",
        "        # print(label.shape, 'label')\n",
        "        x_pos = x_pos.to(device)\n",
        "        x_neg = x_neg.to(device)\n",
        "        flat_pos = self.forward(x_pos)\n",
        "        flat_neg = self.forward(x_neg)\n",
        "\n",
        "        next_pos.append(flat_pos)\n",
        "        next_neg.append(flat_neg)\n",
        "        next_label.append(label)\n",
        "\n",
        "        x_pos = x_pos.detach()\n",
        "        x_neg = x_neg.detach()\n",
        "\n",
        "        # print(flat_pos.shape, 'xpos')\n",
        "        # print(flat_neg.shape, 'xneg')\n",
        "\n",
        "      next_pos = torch.cat(next_pos, dim=0)\n",
        "      next_neg = torch.cat(next_neg, dim=0)\n",
        "      next_label = torch.cat(next_label, dim=0)\n",
        "\n",
        "      return DataLoader(TensorDataset(\n",
        "        next_pos,\n",
        "        next_neg,\n",
        "        next_label\n",
        "    ), batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "\n",
        "class MaxPool_layer(nn.MaxPool2d):\n",
        "\n",
        "  def train_ff(self,train_loader, epoch_range, batch_size, lr=0.03, threshold=2.0):\n",
        "    next_pos = []\n",
        "    next_neg = []\n",
        "    next_label = []\n",
        "    for (x_pos, x_neg, label) in train_loader:\n",
        "        # print(x_pos.shape, 'xpos')\n",
        "        # print(x_neg.shape, 'xneg')\n",
        "        # print(label.shape, 'label')\n",
        "        x_pos = x_pos.to(device)\n",
        "        x_neg = x_neg.to(device)\n",
        "        flat_pos = self.forward(x_pos)\n",
        "        flat_neg = self.forward(x_neg)\n",
        "\n",
        "        next_pos.append(flat_pos)\n",
        "        next_neg.append(flat_neg)\n",
        "        next_label.append(label)\n",
        "\n",
        "        x_pos = x_pos.detach()\n",
        "        x_neg = x_neg.detach()\n",
        "\n",
        "        # print(flat_pos.shape, 'xpos')\n",
        "        # print(flat_neg.shape, 'xneg')\n",
        "\n",
        "    next_pos = torch.cat(next_pos, dim=0)\n",
        "    next_neg = torch.cat(next_neg, dim=0)\n",
        "    next_label = torch.cat(next_label, dim=0)\n",
        "\n",
        "    return DataLoader(TensorDataset(\n",
        "      next_pos,\n",
        "      next_neg,\n",
        "      next_label\n",
        "  ), batch_size=batch_size, shuffle=True)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGgu0adLdP8P"
      },
      "outputs": [],
      "source": [
        "class Linear_layer(nn.Linear):\n",
        "\n",
        "  def __init__(self, in_features, out_features,\n",
        "                 bias=True, device=None, dtype=None):\n",
        "    super().__init__(in_features, out_features, bias, device, dtype)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "      # print(x_direction.shape)\n",
        "      return self.relu(\n",
        "          torch.mm(x_direction, self.weight.T) +\n",
        "          self.bias.unsqueeze(0))\n",
        "\n",
        "  def train_ff(self, train_loader, epoch_range, batch_size, lr=0.03, threshold=2.0):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "    next_pos = []\n",
        "    next_neg = []\n",
        "    next_label = []\n",
        "    # for (x_pos, x_neg, label) in train_loader:\n",
        "    #   x_pos = x_pos.to(device)\n",
        "    #   x_neg = x_neg.to(device)\n",
        "      # for i in epoch_range:\n",
        "    for i in epoch_range:\n",
        "      \n",
        "      for (x_pos, x_neg, label) in train_loader:\n",
        "        x_pos = x_pos.to(device)\n",
        "        x_neg = x_neg.to(device)\n",
        "        g_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "        g_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "          \n",
        "        loss = torch.log(1 + torch.exp(torch.cat([\n",
        "            -g_pos + threshold,\n",
        "            g_neg - threshold]))).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        x_pos = x_pos.detach()\n",
        "        x_neg = x_neg.detach()\n",
        "    \n",
        "    for (x_pos, x_neg, label) in train_loader:\n",
        "      x_pos = x_pos.to(device)\n",
        "      x_neg = x_neg.to(device)  \n",
        "      x_pos_next = self.forward(x_pos).detach()\n",
        "      x_neg_next = self.forward(x_neg).detach()\n",
        "      next_pos.append(x_pos_next)\n",
        "      next_neg.append(x_neg_next)\n",
        "      next_label.append(label)\n",
        "      x_pos = x_pos.detach()\n",
        "      x_neg = x_neg.detach()\n",
        "  \n",
        "    next_pos = torch.cat(next_pos, dim=0)\n",
        "    next_neg = torch.cat(next_neg, dim=0)\n",
        "    next_label = torch.cat(next_label, dim=0)\n",
        "    \n",
        "    return DataLoader(TensorDataset(\n",
        "        next_pos,\n",
        "        next_neg,\n",
        "        next_label\n",
        "    ), batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxf6VH5ZfB6X"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self, layers, num_labels):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.num_labels = num_labels\n",
        "\n",
        "  def train_ff(self, train_loader, epochs=1000, **kwargs):\n",
        "    cur_train_loader = train_loader\n",
        "    batch_size = train_loader.batch_size\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      \n",
        "      is_large_batch = len(cur_train_loader) >= 5\n",
        "      print(f\"Training layer: {i+1} ... tqdm: {'loader' if not is_large_batch else 'epoch'}\")\n",
        "\n",
        "      cur_train_loader = tqdm(cur_train_loader) if not is_large_batch else cur_train_loader\n",
        "      epoch_range = tqdm(range(epochs)) if is_large_batch else range(epochs)\n",
        "      cur_train_loader = layer.train_ff(cur_train_loader, epoch_range=epoch_range, batch_size=batch_size, **kwargs)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  def predict_ff(self, data_loader):\n",
        "\n",
        "    def predict(layers, x, num_labels):\n",
        "      goodness_per_label = []\n",
        "      for label in range(num_labels):\n",
        "          h = overlay_y_on_x(x, label, num_labels)\n",
        "          goodness = []\n",
        "          for i, layer in enumerate(layers):\n",
        "                h = layer(h)\n",
        "                if(i != 0 and i !=1 and i != 3 and i != 4):\n",
        "                  goodness += [h.pow(2).mean(1)]\n",
        "          goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
        "      goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "      return goodness_per_label.argmax(1)\n",
        "    \n",
        "    preds = []\n",
        "    labels = []\n",
        "    for x, label in data_loader:\n",
        "      x = x.to(device)\n",
        "      preds.append(predict(self.layers, x, self.num_labels))\n",
        "      labels.append(label)\n",
        "\n",
        "    preds = torch.cat(preds, 0)\n",
        "    labels = torch.cat(labels, 0)\n",
        "    return preds.cpu(), labels.cpu()\n",
        "\n",
        "  def predict_bp(self, data_loader):\n",
        "    preds = []\n",
        "    labels = []\n",
        "    for input, label in data_loader:\n",
        "      input = input.to(device)\n",
        "      pred = self.forward(input)\n",
        "      preds.append(pred.argmax(1))\n",
        "      labels.append(label)\n",
        "    preds = torch.cat(preds, 0)\n",
        "    labels = torch.cat(labels,0)\n",
        "    return preds.cpu(), labels.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvsnu3DnkRoz",
        "outputId": "8a064437-94d3-49bc-c0fe-f5398e69b417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data/MNIST/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
            "               Lambda()\n",
            "           ) Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data/MNIST/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
            "               Lambda()\n",
            "           )\n"
          ]
        }
      ],
      "source": [
        "def MNIST_dataset():\n",
        "  transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0.1307,), (0.3081,)),\n",
        "        Lambda(lambda x: torch.flatten(x))])\n",
        "  train_data = MNIST('./data/MNIST/', train=True,\n",
        "              download=True,\n",
        "              transform=transform)\n",
        "  test_data = MNIST('./data/MNIST/', train=False,\n",
        "              download=True,\n",
        "              transform=transform)\n",
        "  return train_data, test_data\n",
        "\n",
        "mnist_train_data, mnist_test_data = MNIST_dataset()\n",
        "print(mnist_train_data, mnist_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bTBFdAokYlv"
      },
      "outputs": [],
      "source": [
        "def create_ff_train_dataset(train_loader, num_labels):\n",
        "  pos_set = []\n",
        "  neg_set = []\n",
        "  label_set = []\n",
        "  for input, label in tqdm(train_loader):\n",
        "    x_pos = overlay_y_on_x(input, label, num_labels)\n",
        "    rnd = torch.randperm(input.size(0))\n",
        "    x_neg = overlay_y_on_x(input, label[rnd], num_labels)\n",
        "    pos_set.append(x_pos)\n",
        "    neg_set.append(x_neg)\n",
        "    label_set.append(label)\n",
        "  pos_set = torch.cat(pos_set, 0)\n",
        "  neg_set = torch.cat(neg_set, 0)\n",
        "  label_set = torch.cat(label_set, 0)\n",
        "  return TensorDataset(pos_set, neg_set, label_set)\n",
        "\n",
        "def create_ff_val_dataset(val_loader):\n",
        "  inputs = []\n",
        "  labels = []\n",
        "\n",
        "  for input, label in tqdm(val_loader):\n",
        "    inputs.append(input)\n",
        "    labels.append(label)\n",
        "  inputs = torch.cat(inputs, 0)\n",
        "  labels = torch.cat(labels, 0)\n",
        "  return TensorDataset(inputs, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm7iKjVZkxu-",
        "outputId": "38589398-95a3-473a-c40f-f8ba922275bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 59/59 [00:14<00:00,  3.97it/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = create_ff_train_dataset(DataLoader(mnist_train_data, batch_size=1024, shuffle=False), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z94S7Vi8kxu_",
        "outputId": "64e7ea1d-ec32-43b4-9aa2-2d55b99096fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:02<00:00,  4.08it/s]\n"
          ]
        }
      ],
      "source": [
        "test_dataset = create_ff_val_dataset(DataLoader(mnist_test_data, batch_size=1024, shuffle=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Fqact4lBCw"
      },
      "outputs": [],
      "source": [
        "layers = [\n",
        "    # Convolutional_layer((28, 28, 1), 6, 3),\n",
        "    # Flatten_layer(),\n",
        "    # Linear_layer(26*26*6, 1225, device=device),\n",
        "    # Convolutional_layer((35,35,1),16,5),\n",
        "    # Flatten_layer(),\n",
        "    # Linear_layer(31*31*16, 500, device=device)\n",
        "    Convolutional_layer((28, 28, 1), 6, kernel_size = 5, padding=2),\n",
        "    MaxPool_layer(kernel_size=2, stride=2),\n",
        "    Flatten_layer(),\n",
        "    Convolutional_layer((14,14,6),16, kernel_size=5, padding =0),\n",
        "    MaxPool_layer(kernel_size=2, stride=2),\n",
        "    Flatten_layer(),\n",
        "    Linear_layer(400, 2000, device=device),\n",
        "    Linear_layer(2000, 2000, device=device)\n",
        "]\n",
        "net = Net(layers, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CbHSrxwuEm6",
        "outputId": "5ac14b5a-ae1d-49aa-c29e-f7f893f8651a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 1 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [15:40<00:00,  1.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 2 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 3 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 4 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "100%|██████████| 1000/1000 [14:23<00:00,  1.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 5 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 6 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 7 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "100%|██████████| 1000/1000 [14:10<00:00,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training layer: 8 ... tqdm: epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [14:12<00:00,  1.17it/s]\n"
          ]
        }
      ],
      "source": [
        "net.train_ff(DataLoader(train_dataset, batch_size=512, shuffle=False), epochs=1000, lr=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvTIVysRuPJY"
      },
      "outputs": [],
      "source": [
        "pred, true = net.predict_ff(DataLoader(test_dataset, batch_size=512, shuffle=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-8meZMpuPJa",
        "outputId": "1a874743-9abd-42ce-d8ea-725e18e0fa09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1075"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(true, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENrSsPyXuPJa",
        "outputId": "9e38e236-99c8-495c-b513-f49408fd5665"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.21314554, 0.03621399, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.01830664])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(true, pred, average=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVBT4YU9i21o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}